# 자연어 처리

여기서는 Deep-learning 기반 자연어처리 모델에 대해 설명합니다. 

## BERT 

Bidiectional Encoding Representation form Transformers (BERT)는 앞뒤 문맥으로 빈칸의 단어를 예측합니다. 

## GPT

Generative Pre-trained Transformer (GPT)는 이전 문맥을 고려하여 자연스러운 다음 단어를 예측합니다. 

## Reference 

[BERT와 GPT로 배우는 자연어 처리](https://ratsgo.github.io/nlpbook/docs/tutorial_links)
