# Tokenization

토큰화 (Tokenization)는 문장을 토큰 시퀀스(Token Sequence)로 나누는 과정입니다. 

## Token

- 토큰(Token)은 자연어 처리 모델의 입력을 의미 합니다. 
- 토큰은 문장(Sentence)보다 작은 단위입니다. 
- 한 문장은 여러 개의 토큰으로 구성됩니다.
- 토큰 분리 기준은 상황에 따라 다를 수 있습니다. 
- 문장을 띄어쓰기만으로 나눌수도 있고, 의미의 최소 단위인 형태소(Morpheme) 단위로 나눌 수도 있습니다. 

## Tokenizer 

- 토크나이저 (Tokenizer)는 토큰화를 수행하는 프로그램입니다. 
- BPE (Byte Pair Encoding), 워드피스(ㅈWorkpiece) 등이 있습니다. 
- 대표적인 한국어 토크나이저로는 은전한닢(mecab), 꼬꼬마(kkma)등이 있습니다. 
  - 언어 전문가들이 토큰화해 놓은 데이터를 학습에 최대한 전문적인 분석 결과와 비슷하게 토큰화를 수행합니다. 
  - 토큰화뿐 아니라 품사부착(Part-Of-Speech tagging)까지 수행할 수 있습니다. 


## 토큰화 기법

### BPE

- BPE(Byte-Pair Encoding)은 원래 정보를 압축하는 알고리즘으로 제안되었는데, 자연어 처리 모델에도 널리 쓰이는 토큰화 기법입니다. 
- GPT 모델은 BPE 기법으로 토큰화를 수행합니다. 
- BPE 어휘 집합은 고빈도 바이그램 쌍을 병합하는 방식으로 구축합니다. 
- 어절별로 병합 우선순위가 높은 바이그램 쌍을 반복해서 병합하고, 병합된 토큰이 어휘 집합에 있는지 확인해 최종 결과를 도출합니다.

### Wordpiece

- BERT 모델은 BPE와 유사한 워드피스(Wordpiece)를 토크나이저로 사용합니다. 
- 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 유사하지만, 단순히 빈도를 기준으로 병합하는것이 아니라, 병합했을 때 말뭉치의 likelihood를 높이는 쌍을 병합합니다. 
- 병합 후보에 오른 쌍을 미리 병합해 보고 잃는 것과 가치 등을 판단한 후에 병합합니다. 
- 병합 대상 전체 후보들 가운데에서 가장 높은 쌍을 합칩니다. 
- 우선 순위를 보지 않고, 어휘 집합만 가지고 토큰화를 수행합니다. 
- 분석 대상 어절에 어휘 집합에 있는 서브워드가 포함돼 있을때, 해당 서브워드를 어절에서 분리합니다. 서브워드가 여럿이 있다면 가장 긴 서브워드를 선택합니다. 
- 분석 대상 문자열에서 서브워드 후보가 하나도 없으면, 해당 문자열 전체를 미등록 단어로 취급합니다. 


