# 트랜스포머(Transformer)

## Transformer

- 2017년 구글이 제안한 Sequencee-to-sequence 모델입니다.
- 자연어처리에서는 BERT나 GPT 같은 트랜스포머 기반 언어 모델이 좋은 성능을 내고 있습니다.
- 트랜스포머의 학습은 인코더와 디코더 입력이 주어졌을 때 정답에 해당하는 단어의 확률값을 높이는 방식으로 수행됩니다.
 
### Sequencee-to-sequence

- Sequencee-to-sequence란 특정 속성을 가진 시퀀스를 다른 속성의 시퀀스로 변환하는 작업니다. 
- 소스와 타깃의 길이가 다를수 있습니다. 
- Sequencee-to-sequence 태스크를 수행하는 모델은 인코더와 디코더로 구성됩니다. 
  - 인코더는 소스 시퀀스의 정보를 압축해 디코더로 보내는 역할을 담당합니다.
  - 인코더가 소스 시퀀스 정보를 압축하는 과정을 인코딩이라고 합니다.
  - 디코더는 인코더가 보내 준 소스 시퀀스 정보를 받아서 타깃 시퀀스를 생성합니다.
  - 디코더가 타깃 시퀀스를 생성하는 과정을 디코딩이라고 합니다. 
  - 디코더 출력은 타깃 언어의 어휘수만큼의 차원으로 구성된 백터이며 백터의 element값은 모두 확률입니다. 
  - 타깃이 3만개의 어휘라면, 디코더 출력은 3만차원의 백터입니다. 이때 확률이므로 모두 합치면 1이 됩니다. 
- 자연어에서 시퀀스는 단어 또는 형태소의 나열입니다.  

## Transfomer Blocks

![image](https://user-images.githubusercontent.com/52392004/208555018-9c0a12ae-c6e4-4345-be55-bccc2baad2dd.png)

### Multi-head Attention

- 셀프 어텐션을 동시에 여러번 수행하는것을 가리킵니다. 

### Feedforward Neural Network

- 피드포워드 신경망의 입력은 현재 블록의 멀티헤드 어텐션의 개별 출력 백터입니다. 멀티 헤드 어텐션의 출력은 입력 단어들에 대응하는 벡터 시퀀스입니다. 
- 피드포워드 신경망의 활성함수는 ReLU (Rectified Linear Unit)입니다. 

### Residual Connection

- Transfomer Blocks 다이어그램의 "Add"에 해당합니다. 
- 잔차 연결(Residual Connection)은 블록이나 레이어 계산을 건너뛰는 경로를 하나 두는것을 말합니다. 
- 딥러닝 모델에서 레이어가 많아지면 모델을 업데이트하기 위한 신호(Gradient)가 전달되는 경로가 길어지기 때문에 학습이 어려운 경향이 있습니다. 
- 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 쉽게하는 효과가 있습니다. 

### Layer Normalization

- Transfomer Blocks 다이어그램의 "Norm"에 해당합니다.
- 레이어 정규화(Layer Normalization)란 미니 배치의 인스턴스별로 평균을 빼주고 표준편차를 나눠 정규화를 수행하는 기법입니다. 
- 레이어 정규화를 수행하면 학습이 안정되고 속도가 빨라지는 효과가 있습니다. 


## Attention

- 어텐션은 시퀸스 입력에 수행하는 기계학습 방법의 일종입니다. 
- 어텐션은 시퀀스 요소 가운데 중요한 요소에 집중하고 나머지는 무시해 태스크 수행 성능을 끌어올립니다. 
- 어텐션을 기계 번역 과제에 도입한다면 타깃 언어를 디코딩할 때 소스 언어의 단어 시퀀스 가운데 디코딩에 도움이 되는 단어 위주로 취사 선택하여 번역 품질을 끌어올리게 됩니다. 즉, 어텐션은 디코딩할 때 소스 시퀀스 가운데 중요한 요소만 추립니다. 

### Self Attention
- 트랜스포머 구조에서 멀티헤드 어텐션은 Self Attention으로도 불립니다. 
- 트랜스포머 성능과 관련되어 매우 중요합니다.
- 자신에게 수행하는 어텐션 기법입니다.
- 입력 시퀀스 가운데 태스크 수행에 의미 있는 요소들 위주로 정보를 추출합니다. 
- 셀프 어텐션 수행대상은 입력 시퀀스 전체입니다. 
- 개별 단어와 전체 입력 시퀀스를 대상으로 어텐션 계산을 수행해 문맥 전체를 고려하므로, 지역적인 문맥만 보는 CNN보다 강점이 있습니다. 
- 시퀀스 길이가 길어지더라도 정보를 잊거나 왜곡할 염려가 없습니다. 
- 어텐션은 소스 시퀀스 전체 단어들과 타깃 시퀀스 단어 하나 사이를 연결하지만, 셀프 어텐션은 입력 시퀀스 전체 단어들 사이를 연결합니다. 
- 어텐션은 RNN 구조 위에서 동작하지만 셀프 어텐션은 RNN 없이 동작합니다. 
- 타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만, 셍프 어텐션은 인코더, 디코더 블럭의 개수만큼 반복 수행합니다. 
- 트랜스포머 인코더/디코더 모두 셀프 어텐션이 적용됩니다. 
- 셀프 어텐션은 쿼리(Query), 키(Key), 값(Value)이 서로 영향을 주고 받으면서 문장의 의미를 계산합니다. 
- 셀프 어텐션 모듈은 Value vactor들을 가중합하는 방식으로 계산합니다. 

#### 다른 방식과 비교
- CNN에서는 합성곱 필터의 크기를 넘어서는 문맥은 읽어내기 어렵습니다.
- RNN은 CNN에 비해 시퀀스 정보를 압축하는데 강점이 있지만 시퀀스 길이가 길어 질수록 정보 압축에 문제가 발생하여, 오래전에 입력된 단어는 잊어버리거나, 특정 단어 정보를 과도하게 반영해 정보 전체를 왜곡하는 경우가 생길수 있습니다. 
- 어텐션은 디코더쪽 RNN에 어텐션을 추가하는 방식을 초반에 입력된 단어가 잊어버리는 현상을 해결합니다. 
- 어텐션은 디코더가 타깃 시퀀스를 생성할 때, 소스 시퀀스 전체에서 어떤 요소에 주목해야 할지를 알려줘서, 번역 품질이 떨어지는것을 막을 수 있습니다. 


