# 트랜스포머(Transformer)

## Transformer

- 2017년 구글이 제안한 Sequencee-to-sequence 모델입니다.
- 자연어처리에서는 BERT나 GPT 같은 트랜스포머 기반 언어 모델이 좋은 성능을 내고 있습니다.
- 트랜스포머의 학습은 인코더와 디코더 입력이 주어졌을 때 정답에 해당하는 단어의 확률값을 높이는 방식으로 수행됩니다.
 
### Sequencee-to-sequence

- Sequencee-to-sequence란 특정 속성을 가진 시퀀스를 다른 속성의 시퀀스로 변환하는 작업니다. 
- 소스와 타깃의 길이가 다를수 있습니다. 
- Sequencee-to-sequence 태스크를 수행하는 모델은 인코더와 디코더로 구성됩니다. 
  - 인코더는 소스 시퀀스의 정보를 압축해 디코더로 보내는 역할을 담당합니다.
  - 인코더가 소스 시퀀스 정보를 압축하는 과정을 인코딩이라고 합니다.
  - 디코더는 인코더가 보내 준 소스 시퀀스 정보를 받아서 타깃 시퀀스를 생성합니다.
  - 디코더가 타깃 시퀀스를 생성하는 과정을 디코딩이라고 합니다. 
  - 디코더 출력은 타깃 언어의 어휘수만큼의 차원으로 구성된 백터이며 백터의 element값은 모두 확률입니다. 
  - 타깃이 3만개의 어휘라면, 디코더 출력은 3만차원의 백터입니다. 이때 확률이므로 모두 합치면 1이 됩니다. 

## Transfomer Blocks

### Multi-head Attention

### Feedforward Neural Network

### Residual Connection

### Layer Normalization



## Attention

### Attention

- 어텐션은 시퀸스 입력에 수행하는 기계학습 방법의 일종입니다. 
- 어텐션은 시퀀스 요소 가운데 중요한 요소에 집중하고 나머지는 무시해 태스크 수행 성능을 끌어올립니다. 
- 어텐션을 기계 번역 과제에 도입한다면 타깃 언어를 디코딩할 때 소스 언어의 단어 시퀀스 가운데 디코딩에 도움이 되는 단어 위주로 취사 선택하여 번역 품질을 끌어올리게 됩니다. 즉, 어텐션은 디코딩할 때 소스 시퀀스 가운데 중요한 요소만 추립니다. 

### Self Attention
- 트랜스포머 구조에서 멀티헤드 어텐션은 Self Attention으로도 불립니다. 
- 트랜스포머 성능과 관련되어 매우 중요합니다.

