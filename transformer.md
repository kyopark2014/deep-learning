# 트랜스포머(Transformer)

## Transformer

- 2017년 구글이 제안한 Sequencee-to-sequence 모델입니다.
- 자연어처리에서는 BERT나 GPT 같은 트랜스포머 기반 언어 모델이 좋은 성능을 내고 있습니다.
- 트랜스포머의 학습은 인코더와 디코더 입력이 주어졌을 때 정답에 해당하는 단어의 확률값을 높이는 방식으로 수행됩니다.
 
### Sequencee-to-sequence

- Sequencee-to-sequence란 특정 속성을 가진 시퀀스를 다른 속성의 시퀀스로 변환하는 작업니다. 
- 소스와 타깃의 길이가 다를수 있습니다. 
- Sequencee-to-sequence 태스크를 수행하는 모델은 인코더와 디코더로 구성됩니다. 
  - 인코더는 소스 시퀀스의 정보를 압축해 디코더로 보내는 역할을 담당합니다.
  - 인코더가 소스 시퀀스 정보를 압축하는 과정을 인코딩이라고 합니다.
  - 디코더는 인코더가 보내 준 소스 시퀀스 정보를 받아서 타깃 시퀀스를 생성합니다.
  - 디코더가 타깃 시퀀스를 생성하는 과정을 디코딩이라고 합니다. 
  - 디코더 출력은 타깃 언어의 어휘수만큼의 차원으로 구성된 백터이며 백터의 element값은 모두 확률입니다. 
  - 타깃이 3만개의 어휘라면, 디코더 출력은 3만차원의 백터입니다. 이때 확률이므로 모두 합치면 1이 됩니다. 

